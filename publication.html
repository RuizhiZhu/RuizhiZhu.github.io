<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Ruizhi Zhu (M.S. student at Minzu University of China)</title>
    <base href="https://ruizhizhu.github.io/publication.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Ruizhi Zhu (M.S. student at Minzu University of China)</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
    <!-- <div class="menu-item"><a href="software.html">Software</a></div> -->
    <div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
    <h1 style="margin-top: 0em">Publications</a></h1><br>
    <p>An asterisk (*) beside authors' names indicates equal contributions.</p>

    <div>
        <h2><hr>Preprints</h2>
        <ul>
            <li><p>
<!--
                <b>MH Guo</b>, ZN Liu, TJ Mu, SM Hu.<br>
                Beyond self-attention: External attention using two linear layers for visual tasks.<br>
                [<a href="https://arxiv.org/pdf/2105.02358.pdf" target="_blank">arXiv</a>][<A HREF="https://github.com/MenghaoGuo/EANet">CODE</A>]
-->
            </p></li>
            <li><p>
<!--                
                SM Hu, ZN Liu, <b>MH Guo</b>, JX Cai, J Huang, TJ Mu, RR Martin.<br>
                Subdivision-Based Mesh Convolution Networks.<br>
                [<a href="https://arxiv.org/pdf/2106.00455.pdf" target="_blank">arXiv</a>][<A HREF="https://github.com/lzhengning/SubdivNet">CODE</A>]
-->
            </p></li>            
        </ul>
    </div>

    <div>
        <h2><hr><a name="conference"></a>Conference Papers</h2>
        <ol>
            <li><p>
                Pathological Voice Recognition Based on Multi-Feature Fusion <br>
                <b>RZ Zhu</b>, RX Li, JY Li, Y Liu, Y Liu, JR Li.<br>
                In <A HREF="https://www.servicessociety.org/iccc">ICCC</A>, 2023. <b>Accepted</b><br>
            </p></li>    
            <li><p>
<!--                
                Is Attention Better Than Matrix Decomposition? <br>
                Z Geng*, <b>MH Guo*</b>, H Chen, X Li, K Wei, Z Lin.<br>
                In <A HREF="https://openreview.net/forum?id=1FvkSpWosOl">ICLR</A>, 2021.<br>
                [<A HREF="https://openreview.net/pdf?id=1FvkSpWosOl">PDF</A>][<A HREF="https://github.com/Gsunshine/Enjoy-Hamburger">CODE</A>]
-->
            </p></li>
        </ol>
    </div>
    
    <div>
        <h2><hr><a name="journal"></a>Journal Papers</h2>
        <ol>
            <li><p>
<!--                
                Pct: Point cloud transformer <br>
                <b>MH Guo</b> , JX Cai, ZN Liu, TJ Mu, RR Martin, SM Hu.<br>
                In CVMJ, 2021.
                [<A HREF="https://arxiv.org/pdf/2012.09688.pdf">PDF</A>][<A HREF="https://github.com/MenghaoGuo/PCT">CODE</A>]
-->
            </p></li>
            <li><p>
<!--                
                Can Attention Enable MLPs To Catch Up With CNNs? <br>
                <b>MH Guo</b>, ZN Liu, TJ Mu, D Liang, RR Martin, SM Hu <br>
                In CVMJ, 2021.
                [<A HREF="https://arxiv.org/pdf/2105.15078.pdf">PDF</A>]
-->
            </p></li>
        </ol>
    </div>

    
    
</td>
</tr>
</table>
</body>
</html>
